{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа по теме \"Dataset, Dataloader, BatchNorm, Dropout, Оптимизация\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__\n",
    "\n",
    "1. Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)\n",
    "2. Обернуть его в Dataloader\n",
    "3. Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "4. Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "california_housing.frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "california_housing.frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обернем датасет в Dataloader\n",
    "X_train = torch.tensor(X_train.values)\n",
    "y_train = torch.tensor(y_train.values).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test.values)\n",
    "y_test = torch.tensor(y_test.values).reshape(-1, 1)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512,\n",
    "                                          shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512,\n",
    "                                          shuffle=False)\n",
    "                            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построим модель"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас имеется 8 пизнаков и 1 таргет (цена). Следовательно, построим регрессионную модель. Также бужем использовать метод dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        raise RuntimeError\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, hidden_dim1, 'relu')\n",
    "        #self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dp = nn.Dropout(0.10)\n",
    "        self.fc2 = Perceptron(hidden_dim1, hidden_dim2, 'relu')\n",
    "        self.fc3 = Perceptron(hidden_dim2, hidden_dim3, 'relu')\n",
    "        self.fc4 = Perceptron(hidden_dim3, 1, \"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        #x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.dp(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = self.dp(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FeedForward(8, 24, 12, 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь и оптимайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')  # mean square error\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.00001)\n",
    "#optimizer =  torch.optim.RMSprop(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs=200, optimizer=torch.optim.RMSprop(net.parameters(), lr=0.0001)):\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            batch_inputs, batch_labels = data[0], data[1]\n",
    "            inputs, labels = batch_inputs[i].float(), batch_labels[i].float()\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "            if i % 5000 == 0:\n",
    "                net.eval()\n",
    "                print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                    (epoch + 1, num_epochs, running_loss))\n",
    "                running_loss = 0.0\n",
    "                running_loss_test = 0.0\n",
    "                for num, data2 in enumerate(test_loader):\n",
    "                    batch_test_inputs, batch_test_labels = data2[0], data2[1]\n",
    "                    test_inputs, test_labels = batch_test_inputs[num].float(), batch_test_labels[num].float()\n",
    "                    test_outputs = net(test_inputs)\n",
    "                    test_lost = criterion(test_outputs, test_labels)\n",
    "                    running_loss_test += test_lost.item()\n",
    "                print(f'Test err: {running_loss_test}')\n",
    "                net.train()\n",
    "\n",
    "    print('Training is finishing!')\n",
    "    return running_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [2]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [3]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [4]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [5]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [6]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [7]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [8]/[200] running accumulative loss across all batches: 3.743\n",
      "Test err: 41.310118943452835\n",
      "Epoch [9]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [10]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [11]/[200] running accumulative loss across all batches: 0.787\n",
      "Test err: 40.499452501535416\n",
      "Epoch [12]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [13]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [14]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [15]/[200] running accumulative loss across all batches: 6.261\n",
      "Test err: 41.310118943452835\n",
      "Epoch [16]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.25883451104164\n",
      "Epoch [17]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 40.68960466980934\n",
      "Epoch [18]/[200] running accumulative loss across all batches: 5.771\n",
      "Test err: 39.84523019194603\n",
      "Epoch [19]/[200] running accumulative loss across all batches: 6.304\n",
      "Test err: 34.74740979075432\n",
      "Epoch [20]/[200] running accumulative loss across all batches: 3.090\n",
      "Test err: 30.945740990166087\n",
      "Epoch [21]/[200] running accumulative loss across all batches: 3.129\n",
      "Test err: 30.24784310493851\n",
      "Epoch [22]/[200] running accumulative loss across all batches: 1.002\n",
      "Test err: 32.16467443108559\n",
      "Epoch [23]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 25.489589849486947\n",
      "Epoch [24]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 25.65331383422017\n",
      "Epoch [25]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 24.678381733596325\n",
      "Epoch [26]/[200] running accumulative loss across all batches: 2.310\n",
      "Test err: 23.66161973774433\n",
      "Epoch [27]/[200] running accumulative loss across all batches: 2.346\n",
      "Test err: 23.250143049284816\n",
      "Epoch [28]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 30.509309391491115\n",
      "Epoch [29]/[200] running accumulative loss across all batches: 1.244\n",
      "Test err: 27.26583605335327\n",
      "Epoch [30]/[200] running accumulative loss across all batches: 1.215\n",
      "Test err: 28.321318731585052\n",
      "Epoch [31]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 34.995340280234814\n",
      "Epoch [32]/[200] running accumulative loss across all batches: 1.983\n",
      "Test err: 25.491090530063957\n",
      "Epoch [33]/[200] running accumulative loss across all batches: 1.384\n",
      "Test err: 22.891672156751156\n",
      "Epoch [34]/[200] running accumulative loss across all batches: 1.494\n",
      "Test err: 24.770545344101265\n",
      "Epoch [35]/[200] running accumulative loss across all batches: 0.092\n",
      "Test err: 22.589367667213082\n",
      "Epoch [36]/[200] running accumulative loss across all batches: 0.001\n",
      "Test err: 25.319791266745597\n",
      "Epoch [37]/[200] running accumulative loss across all batches: 0.514\n",
      "Test err: 24.251041927491315\n",
      "Epoch [38]/[200] running accumulative loss across all batches: 2.280\n",
      "Test err: 27.792647919617593\n",
      "Epoch [39]/[200] running accumulative loss across all batches: 1.511\n",
      "Test err: 22.74097395176068\n",
      "Epoch [40]/[200] running accumulative loss across all batches: 0.976\n",
      "Test err: 24.203438456787808\n",
      "Epoch [41]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 28.567503985017538\n",
      "Epoch [42]/[200] running accumulative loss across all batches: 0.481\n",
      "Test err: 33.83203453198075\n",
      "Epoch [43]/[200] running accumulative loss across all batches: 0.663\n",
      "Test err: 26.683832159265876\n",
      "Epoch [44]/[200] running accumulative loss across all batches: 0.016\n",
      "Test err: 20.323486862704158\n",
      "Epoch [45]/[200] running accumulative loss across all batches: 0.633\n",
      "Test err: 25.352157978340983\n",
      "Epoch [46]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 20.496501143090427\n",
      "Epoch [47]/[200] running accumulative loss across all batches: 0.828\n",
      "Test err: 19.831621518358588\n",
      "Epoch [48]/[200] running accumulative loss across all batches: 5.641\n",
      "Test err: 20.009120726957917\n",
      "Epoch [49]/[200] running accumulative loss across all batches: 1.143\n",
      "Test err: 23.55032971862238\n",
      "Epoch [50]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 19.14014468807727\n",
      "Epoch [51]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 18.943973899862613\n",
      "Epoch [52]/[200] running accumulative loss across all batches: 0.233\n",
      "Test err: 19.013276061043143\n",
      "Epoch [53]/[200] running accumulative loss across all batches: 0.561\n",
      "Test err: 20.667211030460976\n",
      "Epoch [54]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 18.62835129619816\n",
      "Epoch [55]/[200] running accumulative loss across all batches: 1.761\n",
      "Test err: 19.32051030313596\n",
      "Epoch [56]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 19.30176663212478\n",
      "Epoch [57]/[200] running accumulative loss across all batches: 1.955\n",
      "Test err: 18.3824828453362\n",
      "Epoch [58]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 18.834698118269444\n",
      "Epoch [59]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 19.877728069001023\n",
      "Epoch [60]/[200] running accumulative loss across all batches: 0.009\n",
      "Test err: 19.537388205659227\n",
      "Epoch [61]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 20.13306844374165\n",
      "Epoch [62]/[200] running accumulative loss across all batches: 1.764\n",
      "Test err: 18.590135937323794\n",
      "Epoch [63]/[200] running accumulative loss across all batches: 1.365\n",
      "Test err: 20.122582942713052\n",
      "Epoch [64]/[200] running accumulative loss across all batches: 0.415\n",
      "Test err: 17.81005997955799\n",
      "Epoch [65]/[200] running accumulative loss across all batches: 0.292\n",
      "Test err: 17.911447404883802\n",
      "Epoch [66]/[200] running accumulative loss across all batches: 1.074\n",
      "Test err: 21.664350376697257\n",
      "Epoch [67]/[200] running accumulative loss across all batches: 2.275\n",
      "Test err: 20.556311963649932\n",
      "Epoch [68]/[200] running accumulative loss across all batches: 1.112\n",
      "Test err: 21.104773045924958\n",
      "Epoch [69]/[200] running accumulative loss across all batches: 0.687\n",
      "Test err: 21.108246483432595\n",
      "Epoch [70]/[200] running accumulative loss across all batches: 1.340\n",
      "Test err: 21.236973168561235\n",
      "Epoch [71]/[200] running accumulative loss across all batches: 0.815\n",
      "Test err: 23.950287505984306\n",
      "Epoch [72]/[200] running accumulative loss across all batches: 0.009\n",
      "Test err: 19.76723802415654\n",
      "Epoch [73]/[200] running accumulative loss across all batches: 2.067\n",
      "Test err: 19.336959190666676\n",
      "Epoch [74]/[200] running accumulative loss across all batches: 1.376\n",
      "Test err: 18.77866710536182\n",
      "Epoch [75]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 18.767759321723133\n",
      "Epoch [76]/[200] running accumulative loss across all batches: 0.217\n",
      "Test err: 17.42397339566378\n",
      "Epoch [77]/[200] running accumulative loss across all batches: 1.702\n",
      "Test err: 17.37658878556249\n",
      "Epoch [78]/[200] running accumulative loss across all batches: 1.009\n",
      "Test err: 20.723462148569524\n",
      "Epoch [79]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 21.887449491769075\n",
      "Epoch [80]/[200] running accumulative loss across all batches: 7.037\n",
      "Test err: 28.803228039760143\n",
      "Epoch [81]/[200] running accumulative loss across all batches: 0.012\n",
      "Test err: 26.976000552996993\n",
      "Epoch [82]/[200] running accumulative loss across all batches: 0.933\n",
      "Test err: 20.9842779180035\n",
      "Epoch [83]/[200] running accumulative loss across all batches: 0.150\n",
      "Test err: 18.045433044433594\n",
      "Epoch [84]/[200] running accumulative loss across all batches: 0.690\n",
      "Test err: 19.809785794233903\n",
      "Epoch [85]/[200] running accumulative loss across all batches: 2.084\n",
      "Test err: 17.10863458039239\n",
      "Epoch [86]/[200] running accumulative loss across all batches: 0.847\n",
      "Test err: 21.131554344668984\n",
      "Epoch [87]/[200] running accumulative loss across all batches: 0.094\n",
      "Test err: 17.484583478420973\n",
      "Epoch [88]/[200] running accumulative loss across all batches: 0.866\n",
      "Test err: 20.602126754820347\n",
      "Epoch [89]/[200] running accumulative loss across all batches: 0.988\n",
      "Test err: 19.332013397244737\n",
      "Epoch [90]/[200] running accumulative loss across all batches: 1.127\n",
      "Test err: 24.127592702396214\n",
      "Epoch [91]/[200] running accumulative loss across all batches: 0.009\n",
      "Test err: 19.8028560616076\n",
      "Epoch [92]/[200] running accumulative loss across all batches: 1.158\n",
      "Test err: 19.06103432038799\n",
      "Epoch [93]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 16.609954704530537\n",
      "Epoch [94]/[200] running accumulative loss across all batches: 0.850\n",
      "Test err: 19.902528950944543\n",
      "Epoch [95]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 24.596433686092496\n",
      "Epoch [96]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 17.236178359482437\n",
      "Epoch [97]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 19.101704854518175\n",
      "Epoch [98]/[200] running accumulative loss across all batches: 3.674\n",
      "Test err: 17.147676604799926\n",
      "Epoch [99]/[200] running accumulative loss across all batches: 0.108\n",
      "Test err: 19.118599792011082\n",
      "Epoch [100]/[200] running accumulative loss across all batches: 0.244\n",
      "Test err: 19.467873072251678\n",
      "Epoch [101]/[200] running accumulative loss across all batches: 1.316\n",
      "Test err: 16.432885511778295\n",
      "Epoch [102]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 18.848598144948483\n",
      "Epoch [103]/[200] running accumulative loss across all batches: 0.355\n",
      "Test err: 15.770919932518154\n",
      "Epoch [104]/[200] running accumulative loss across all batches: 0.992\n",
      "Test err: 17.76203438930679\n",
      "Epoch [105]/[200] running accumulative loss across all batches: 0.512\n",
      "Test err: 23.31012160377577\n",
      "Epoch [106]/[200] running accumulative loss across all batches: 0.898\n",
      "Test err: 18.484662915579975\n",
      "Epoch [107]/[200] running accumulative loss across all batches: 0.944\n",
      "Test err: 18.011769134551287\n",
      "Epoch [108]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 15.998615943826735\n",
      "Epoch [109]/[200] running accumulative loss across all batches: 2.251\n",
      "Test err: 19.455281687900424\n",
      "Epoch [110]/[200] running accumulative loss across all batches: 1.004\n",
      "Test err: 17.036671746260254\n",
      "Epoch [111]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 16.4904364872491\n",
      "Epoch [112]/[200] running accumulative loss across all batches: 1.589\n",
      "Test err: 14.561622703680769\n",
      "Epoch [113]/[200] running accumulative loss across all batches: 0.724\n",
      "Test err: 15.238808384165168\n",
      "Epoch [114]/[200] running accumulative loss across all batches: 0.087\n",
      "Test err: 14.097409864247311\n",
      "Epoch [115]/[200] running accumulative loss across all batches: 2.689\n",
      "Test err: 14.111162147019058\n",
      "Epoch [116]/[200] running accumulative loss across all batches: 1.107\n",
      "Test err: 14.367955329827964\n",
      "Epoch [117]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 13.99906224157894\n",
      "Epoch [118]/[200] running accumulative loss across all batches: 1.648\n",
      "Test err: 14.146915711986367\n",
      "Epoch [119]/[200] running accumulative loss across all batches: 1.302\n",
      "Test err: 14.883743626996875\n",
      "Epoch [120]/[200] running accumulative loss across all batches: 0.103\n",
      "Test err: 17.518468728289008\n",
      "Epoch [121]/[200] running accumulative loss across all batches: 2.163\n",
      "Test err: 18.853801045566797\n",
      "Epoch [122]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 16.41154382075183\n",
      "Epoch [123]/[200] running accumulative loss across all batches: 0.249\n",
      "Test err: 14.894199723377824\n",
      "Epoch [124]/[200] running accumulative loss across all batches: 1.222\n",
      "Test err: 16.55511927511543\n",
      "Epoch [125]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 17.74804930994287\n",
      "Epoch [126]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 15.802686825860292\n",
      "Epoch [127]/[200] running accumulative loss across all batches: 0.301\n",
      "Test err: 19.54590632781037\n",
      "Epoch [128]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 16.544219967443496\n",
      "Epoch [129]/[200] running accumulative loss across all batches: 0.658\n",
      "Test err: 14.111729578697123\n",
      "Epoch [130]/[200] running accumulative loss across all batches: 1.833\n",
      "Test err: 17.250886217399966\n",
      "Epoch [131]/[200] running accumulative loss across all batches: 0.298\n",
      "Test err: 22.846674511907622\n",
      "Epoch [132]/[200] running accumulative loss across all batches: 0.702\n",
      "Test err: 14.86271468969062\n",
      "Epoch [133]/[200] running accumulative loss across all batches: 0.537\n",
      "Test err: 14.115286579413805\n",
      "Epoch [134]/[200] running accumulative loss across all batches: 0.004\n",
      "Test err: 18.329632142975242\n",
      "Epoch [135]/[200] running accumulative loss across all batches: 0.785\n",
      "Test err: 14.95751477777958\n",
      "Epoch [136]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 13.037325898185372\n",
      "Epoch [137]/[200] running accumulative loss across all batches: 0.157\n",
      "Test err: 12.256725559011102\n",
      "Epoch [138]/[200] running accumulative loss across all batches: 0.047\n",
      "Test err: 17.44520173726778\n",
      "Epoch [139]/[200] running accumulative loss across all batches: 0.818\n",
      "Test err: 14.431567121762782\n",
      "Epoch [140]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 12.102263679727912\n",
      "Epoch [141]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 11.371530710719526\n",
      "Epoch [142]/[200] running accumulative loss across all batches: 0.978\n",
      "Test err: 12.986372009909246\n",
      "Epoch [143]/[200] running accumulative loss across all batches: 0.672\n",
      "Test err: 15.192224497965071\n",
      "Epoch [144]/[200] running accumulative loss across all batches: 0.610\n",
      "Test err: 15.716543803599052\n",
      "Epoch [145]/[200] running accumulative loss across all batches: 0.752\n",
      "Test err: 14.075427155592479\n",
      "Epoch [146]/[200] running accumulative loss across all batches: 1.558\n",
      "Test err: 13.838743558619171\n",
      "Epoch [147]/[200] running accumulative loss across all batches: 1.540\n",
      "Test err: 14.369586712680757\n",
      "Epoch [148]/[200] running accumulative loss across all batches: 0.900\n",
      "Test err: 12.825749613330117\n",
      "Epoch [149]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 11.900050873868167\n",
      "Epoch [150]/[200] running accumulative loss across all batches: 0.535\n",
      "Test err: 15.840588486753404\n",
      "Epoch [151]/[200] running accumulative loss across all batches: 0.563\n",
      "Test err: 15.32853208296001\n",
      "Epoch [152]/[200] running accumulative loss across all batches: 0.279\n",
      "Test err: 17.599946786416695\n",
      "Epoch [153]/[200] running accumulative loss across all batches: 0.192\n",
      "Test err: 18.4605662668273\n",
      "Epoch [154]/[200] running accumulative loss across all batches: 0.609\n",
      "Test err: 14.59520165110007\n",
      "Epoch [155]/[200] running accumulative loss across all batches: 0.140\n",
      "Test err: 12.941197092892253\n",
      "Epoch [156]/[200] running accumulative loss across all batches: 0.979\n",
      "Test err: 11.714947571745142\n",
      "Epoch [157]/[200] running accumulative loss across all batches: 0.132\n",
      "Test err: 12.686974235577509\n",
      "Epoch [158]/[200] running accumulative loss across all batches: 0.489\n",
      "Test err: 14.524828055873513\n",
      "Epoch [159]/[200] running accumulative loss across all batches: 0.080\n",
      "Test err: 13.235409811604768\n",
      "Epoch [160]/[200] running accumulative loss across all batches: 2.596\n",
      "Test err: 10.37259843852371\n",
      "Epoch [161]/[200] running accumulative loss across all batches: 1.610\n",
      "Test err: 10.297596449963748\n",
      "Epoch [162]/[200] running accumulative loss across all batches: 2.304\n",
      "Test err: 10.887902429327369\n",
      "Epoch [163]/[200] running accumulative loss across all batches: 0.114\n",
      "Test err: 12.006464600446634\n",
      "Epoch [164]/[200] running accumulative loss across all batches: 1.336\n",
      "Test err: 16.40674823289737\n",
      "Epoch [165]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 13.912275446229614\n",
      "Epoch [166]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 15.502689172979444\n",
      "Epoch [167]/[200] running accumulative loss across all batches: 1.150\n",
      "Test err: 10.638582631014287\n",
      "Epoch [168]/[200] running accumulative loss across all batches: 7.161\n",
      "Test err: 10.938283201307058\n",
      "Epoch [169]/[200] running accumulative loss across all batches: 1.256\n",
      "Test err: 10.119492480996996\n",
      "Epoch [170]/[200] running accumulative loss across all batches: 0.054\n",
      "Test err: 12.653454054147005\n",
      "Epoch [171]/[200] running accumulative loss across all batches: 0.793\n",
      "Test err: 11.72640159772709\n",
      "Epoch [172]/[200] running accumulative loss across all batches: 0.765\n",
      "Test err: 18.107287714723498\n",
      "Epoch [173]/[200] running accumulative loss across all batches: 0.093\n",
      "Test err: 24.03557115775766\n",
      "Epoch [174]/[200] running accumulative loss across all batches: 1.030\n",
      "Test err: 10.35846783022862\n",
      "Epoch [175]/[200] running accumulative loss across all batches: 0.008\n",
      "Test err: 11.396229974692687\n",
      "Epoch [176]/[200] running accumulative loss across all batches: 0.099\n",
      "Test err: 16.83039502822794\n",
      "Epoch [177]/[200] running accumulative loss across all batches: 0.430\n",
      "Test err: 14.773630480049178\n",
      "Epoch [178]/[200] running accumulative loss across all batches: 0.137\n",
      "Test err: 11.021717500771047\n",
      "Epoch [179]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 12.037291775457561\n",
      "Epoch [180]/[200] running accumulative loss across all batches: 5.140\n",
      "Test err: 13.689436954911798\n",
      "Epoch [181]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 9.862291087396443\n",
      "Epoch [182]/[200] running accumulative loss across all batches: 3.873\n",
      "Test err: 9.484721129585523\n",
      "Epoch [183]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 9.810592865658691\n",
      "Epoch [184]/[200] running accumulative loss across all batches: 0.036\n",
      "Test err: 11.498951562680304\n",
      "Epoch [185]/[200] running accumulative loss across all batches: 0.281\n",
      "Test err: 16.60351333260769\n",
      "Epoch [186]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 12.898841619025916\n",
      "Epoch [187]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 9.047083731622479\n",
      "Epoch [188]/[200] running accumulative loss across all batches: 0.370\n",
      "Test err: 13.333838363061659\n",
      "Epoch [189]/[200] running accumulative loss across all batches: 0.430\n",
      "Test err: 10.640507758944295\n",
      "Epoch [190]/[200] running accumulative loss across all batches: 0.113\n",
      "Test err: 15.056513184390496\n",
      "Epoch [191]/[200] running accumulative loss across all batches: 1.021\n",
      "Test err: 13.253312370743515\n",
      "Epoch [192]/[200] running accumulative loss across all batches: 6.598\n",
      "Test err: 13.076077057833118\n",
      "Epoch [193]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 12.581207737792283\n",
      "Epoch [194]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 9.507020714750979\n",
      "Epoch [195]/[200] running accumulative loss across all batches: 0.144\n",
      "Test err: 8.954143849667162\n",
      "Epoch [196]/[200] running accumulative loss across all batches: 1.044\n",
      "Test err: 9.607590995016835\n",
      "Epoch [197]/[200] running accumulative loss across all batches: 3.784\n",
      "Test err: 8.941478642785114\n",
      "Epoch [198]/[200] running accumulative loss across all batches: 3.289\n",
      "Test err: 8.564994010535884\n",
      "Epoch [199]/[200] running accumulative loss across all batches: 0.289\n",
      "Test err: 10.107036387184053\n",
      "Epoch [200]/[200] running accumulative loss across all batches: 1.402\n",
      "Test err: 10.509002172388136\n",
      "Training is finishing!\n"
     ]
    }
   ],
   "source": [
    "RMSprop_res = train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим оптимайзер на Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [2]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [3]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [4]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [5]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [6]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [7]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [8]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [9]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [10]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [11]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [12]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [13]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [14]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [15]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [16]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [17]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [18]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [19]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [20]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [21]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [22]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [23]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [24]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [25]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [26]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [27]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [28]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [29]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [30]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [31]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [32]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [33]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [34]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [35]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [36]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [37]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [38]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [39]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [40]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [41]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [42]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [43]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [44]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [45]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [46]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [47]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [48]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [49]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [50]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [51]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [52]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [53]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [54]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [55]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [56]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [57]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [58]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [59]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [60]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [61]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [62]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [63]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [64]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [65]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [66]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [67]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [68]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [69]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [70]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [71]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [72]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [73]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [74]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [75]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [76]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [77]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [78]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [79]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [80]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [81]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [82]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [83]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [84]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [85]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [86]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [87]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [88]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [89]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [90]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [91]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [92]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [93]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [94]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [95]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [96]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [97]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [98]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [99]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [100]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [101]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [102]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [103]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [104]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [105]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [106]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [107]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [108]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [109]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [110]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [111]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [112]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [113]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [114]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [115]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [116]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [117]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [118]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [119]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [120]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [121]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [122]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [123]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [124]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [125]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [126]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [127]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [128]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [129]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [130]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [131]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [132]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [133]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [134]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [135]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [136]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [137]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [138]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [139]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [140]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [141]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [142]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [143]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [144]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [145]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [146]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [147]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [148]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [149]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [150]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [151]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [152]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [153]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [154]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [155]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [156]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [157]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [158]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [159]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [160]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [161]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [162]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [163]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [164]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [165]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [166]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [167]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [168]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [169]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [170]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [171]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [172]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [173]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [174]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [175]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [176]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [177]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [178]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [179]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [180]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [181]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [182]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [183]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [184]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [185]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [186]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [187]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [188]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [189]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [190]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [191]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [192]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [193]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [194]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [195]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [196]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [197]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [198]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [199]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Epoch [200]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 41.310118943452835\n",
      "Training is finishing!\n"
     ]
    }
   ],
   "source": [
    "net = FeedForward(8, 24, 12, 6)\n",
    "Adam_res = train(optimizer=torch.optim.Adam(net.parameters(), lr=0.0001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим оптимайзер на SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 39.97309949994087\n",
      "Epoch [2]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 28.650760028511286\n",
      "Epoch [3]/[200] running accumulative loss across all batches: 5.242\n",
      "Test err: 27.83342058956623\n",
      "Epoch [4]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 27.589501881971955\n",
      "Epoch [5]/[200] running accumulative loss across all batches: 5.157\n",
      "Test err: 27.279007574543357\n",
      "Epoch [6]/[200] running accumulative loss across all batches: 5.111\n",
      "Test err: 26.97603945247829\n",
      "Epoch [7]/[200] running accumulative loss across all batches: 5.064\n",
      "Test err: 26.670686283148825\n",
      "Epoch [8]/[200] running accumulative loss across all batches: 5.021\n",
      "Test err: 26.392901240848005\n",
      "Epoch [9]/[200] running accumulative loss across all batches: 4.980\n",
      "Test err: 26.125078715384007\n",
      "Epoch [10]/[200] running accumulative loss across all batches: 4.938\n",
      "Test err: 25.854215297847986\n",
      "Epoch [11]/[200] running accumulative loss across all batches: 4.892\n",
      "Test err: 25.554984013084322\n",
      "Epoch [12]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 25.30455792788416\n",
      "Epoch [13]/[200] running accumulative loss across all batches: 4.814\n",
      "Test err: 25.054763898951933\n",
      "Epoch [14]/[200] running accumulative loss across all batches: 4.774\n",
      "Test err: 24.800498157041147\n",
      "Epoch [15]/[200] running accumulative loss across all batches: 4.732\n",
      "Test err: 24.534647139837034\n",
      "Epoch [16]/[200] running accumulative loss across all batches: 4.696\n",
      "Test err: 24.302685128874145\n",
      "Epoch [17]/[200] running accumulative loss across all batches: 4.654\n",
      "Test err: 24.042154907190707\n",
      "Epoch [18]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 23.79269546850992\n",
      "Epoch [19]/[200] running accumulative loss across all batches: 4.570\n",
      "Test err: 23.512855955218583\n",
      "Epoch [20]/[200] running accumulative loss across all batches: 4.526\n",
      "Test err: 23.238043049517728\n",
      "Epoch [21]/[200] running accumulative loss across all batches: 4.490\n",
      "Test err: 23.01382619551441\n",
      "Epoch [22]/[200] running accumulative loss across all batches: 4.447\n",
      "Test err: 22.74125807371456\n",
      "Epoch [23]/[200] running accumulative loss across all batches: 4.405\n",
      "Test err: 22.484516669181176\n",
      "Epoch [24]/[200] running accumulative loss across all batches: 4.365\n",
      "Test err: 22.23758075456135\n",
      "Epoch [25]/[200] running accumulative loss across all batches: 4.324\n",
      "Test err: 21.981213563587517\n",
      "Epoch [26]/[200] running accumulative loss across all batches: 4.285\n",
      "Test err: 21.744826458394527\n",
      "Epoch [27]/[200] running accumulative loss across all batches: 5.570\n",
      "Test err: 21.509734152350575\n",
      "Epoch [28]/[200] running accumulative loss across all batches: 4.206\n",
      "Test err: 21.264054988510907\n",
      "Epoch [29]/[200] running accumulative loss across all batches: 4.166\n",
      "Test err: 21.02199805341661\n",
      "Epoch [30]/[200] running accumulative loss across all batches: 4.128\n",
      "Test err: 20.79351005703211\n",
      "Epoch [31]/[200] running accumulative loss across all batches: 4.090\n",
      "Test err: 20.56369131617248\n",
      "Epoch [32]/[200] running accumulative loss across all batches: 4.052\n",
      "Test err: 20.33197840768844\n",
      "Epoch [33]/[200] running accumulative loss across all batches: 4.013\n",
      "Test err: 20.103695325553417\n",
      "Epoch [34]/[200] running accumulative loss across all batches: 3.975\n",
      "Test err: 19.878803396597505\n",
      "Epoch [35]/[200] running accumulative loss across all batches: 3.938\n",
      "Test err: 19.65725147537887\n",
      "Epoch [36]/[200] running accumulative loss across all batches: 3.901\n",
      "Test err: 19.438997833058238\n",
      "Epoch [37]/[200] running accumulative loss across all batches: 3.864\n",
      "Test err: 19.22376524284482\n",
      "Epoch [38]/[200] running accumulative loss across all batches: 3.828\n",
      "Test err: 19.011976029723883\n",
      "Epoch [39]/[200] running accumulative loss across all batches: 3.793\n",
      "Test err: 18.80700958147645\n",
      "Epoch [40]/[200] running accumulative loss across all batches: 3.757\n",
      "Test err: 18.601148024201393\n",
      "Epoch [41]/[200] running accumulative loss across all batches: 3.722\n",
      "Test err: 18.398684840649366\n",
      "Epoch [42]/[200] running accumulative loss across all batches: 3.688\n",
      "Test err: 18.199256509542465\n",
      "Epoch [43]/[200] running accumulative loss across all batches: 3.653\n",
      "Test err: 18.002830520272255\n",
      "Epoch [44]/[200] running accumulative loss across all batches: 3.619\n",
      "Test err: 17.80638937279582\n",
      "Epoch [45]/[200] running accumulative loss across all batches: 3.585\n",
      "Test err: 17.61588716134429\n",
      "Epoch [46]/[200] running accumulative loss across all batches: 3.552\n",
      "Test err: 17.428266149014235\n",
      "Epoch [47]/[200] running accumulative loss across all batches: 7.182\n",
      "Test err: 17.251394599676132\n",
      "Epoch [48]/[200] running accumulative loss across all batches: 3.488\n",
      "Test err: 17.0678713619709\n",
      "Epoch [49]/[200] running accumulative loss across all batches: 3.456\n",
      "Test err: 16.88856239616871\n",
      "Epoch [50]/[200] running accumulative loss across all batches: 3.424\n",
      "Test err: 16.711983874440193\n",
      "Epoch [51]/[200] running accumulative loss across all batches: 3.393\n",
      "Test err: 16.53810191899538\n",
      "Epoch [52]/[200] running accumulative loss across all batches: 3.362\n",
      "Test err: 16.366870813071728\n",
      "Epoch [53]/[200] running accumulative loss across all batches: 3.331\n",
      "Test err: 16.198263563215733\n",
      "Epoch [54]/[200] running accumulative loss across all batches: 3.300\n",
      "Test err: 16.03223765641451\n",
      "Epoch [55]/[200] running accumulative loss across all batches: 3.270\n",
      "Test err: 15.86889086663723\n",
      "Epoch [56]/[200] running accumulative loss across all batches: 3.240\n",
      "Test err: 15.707930020987988\n",
      "Epoch [57]/[200] running accumulative loss across all batches: 3.211\n",
      "Test err: 15.549450803548098\n",
      "Epoch [58]/[200] running accumulative loss across all batches: 3.182\n",
      "Test err: 15.393418833613396\n",
      "Epoch [59]/[200] running accumulative loss across all batches: 3.153\n",
      "Test err: 15.239800356328487\n",
      "Epoch [60]/[200] running accumulative loss across all batches: 3.124\n",
      "Test err: 15.08856275677681\n",
      "Epoch [61]/[200] running accumulative loss across all batches: 3.096\n",
      "Test err: 14.939672645181417\n",
      "Epoch [62]/[200] running accumulative loss across all batches: 3.068\n",
      "Test err: 14.793099626898766\n",
      "Epoch [63]/[200] running accumulative loss across all batches: 6.960\n",
      "Test err: 14.645567741245031\n",
      "Epoch [64]/[200] running accumulative loss across all batches: 3.013\n",
      "Test err: 14.503589635714889\n",
      "Epoch [65]/[200] running accumulative loss across all batches: 2.986\n",
      "Test err: 14.363831151276827\n",
      "Epoch [66]/[200] running accumulative loss across all batches: 2.959\n",
      "Test err: 14.226262858137488\n",
      "Epoch [67]/[200] running accumulative loss across all batches: 2.932\n",
      "Test err: 14.090859174728394\n",
      "Epoch [68]/[200] running accumulative loss across all batches: 2.906\n",
      "Test err: 13.957589536905289\n",
      "Epoch [69]/[200] running accumulative loss across all batches: 2.880\n",
      "Test err: 13.824735896661878\n",
      "Epoch [70]/[200] running accumulative loss across all batches: 2.854\n",
      "Test err: 13.695674425922334\n",
      "Epoch [71]/[200] running accumulative loss across all batches: 2.829\n",
      "Test err: 13.568488291464746\n",
      "Epoch [72]/[200] running accumulative loss across all batches: 2.804\n",
      "Test err: 13.443496260792017\n",
      "Epoch [73]/[200] running accumulative loss across all batches: 2.779\n",
      "Test err: 13.320494214072824\n",
      "Epoch [74]/[200] running accumulative loss across all batches: 2.754\n",
      "Test err: 13.199456402100623\n",
      "Epoch [75]/[200] running accumulative loss across all batches: 2.730\n",
      "Test err: 13.079892830457538\n",
      "Epoch [76]/[200] running accumulative loss across all batches: 2.706\n",
      "Test err: 12.962707603815943\n",
      "Epoch [77]/[200] running accumulative loss across all batches: 2.682\n",
      "Test err: 12.847403468098491\n",
      "Epoch [78]/[200] running accumulative loss across all batches: 2.659\n",
      "Test err: 12.733956494834274\n",
      "Epoch [79]/[200] running accumulative loss across all batches: 2.635\n",
      "Test err: 12.62234174227342\n",
      "Epoch [80]/[200] running accumulative loss across all batches: 2.612\n",
      "Test err: 12.512535276822746\n",
      "Epoch [81]/[200] running accumulative loss across all batches: 2.589\n",
      "Test err: 12.404509858111851\n",
      "Epoch [82]/[200] running accumulative loss across all batches: 2.567\n",
      "Test err: 12.298245566547848\n",
      "Epoch [83]/[200] running accumulative loss across all batches: 2.544\n",
      "Test err: 12.193709671264514\n",
      "Epoch [84]/[200] running accumulative loss across all batches: 2.522\n",
      "Test err: 12.09087930538226\n",
      "Epoch [85]/[200] running accumulative loss across all batches: 2.501\n",
      "Test err: 11.989739219105104\n",
      "Epoch [86]/[200] running accumulative loss across all batches: 2.479\n",
      "Test err: 11.8902568857593\n",
      "Epoch [87]/[200] running accumulative loss across all batches: 2.457\n",
      "Test err: 11.792418962334978\n",
      "Epoch [88]/[200] running accumulative loss across all batches: 2.436\n",
      "Test err: 11.696190524575286\n",
      "Epoch [89]/[200] running accumulative loss across all batches: 2.415\n",
      "Test err: 11.601558975162334\n",
      "Epoch [90]/[200] running accumulative loss across all batches: 2.395\n",
      "Test err: 11.508501580363372\n",
      "Epoch [91]/[200] running accumulative loss across all batches: 2.374\n",
      "Test err: 11.416992270271294\n",
      "Epoch [92]/[200] running accumulative loss across all batches: 2.354\n",
      "Test err: 11.327012739959173\n",
      "Epoch [93]/[200] running accumulative loss across all batches: 2.334\n",
      "Test err: 11.238538002595305\n",
      "Epoch [94]/[200] running accumulative loss across all batches: 2.314\n",
      "Test err: 11.151548922876827\n",
      "Epoch [95]/[200] running accumulative loss across all batches: 2.294\n",
      "Test err: 11.0660260762088\n",
      "Epoch [96]/[200] running accumulative loss across all batches: 2.275\n",
      "Test err: 10.981948481407017\n",
      "Epoch [97]/[200] running accumulative loss across all batches: 2.255\n",
      "Test err: 10.899286759551615\n",
      "Epoch [98]/[200] running accumulative loss across all batches: 2.236\n",
      "Test err: 10.818037173710763\n",
      "Epoch [99]/[200] running accumulative loss across all batches: 2.217\n",
      "Test err: 10.738170785363764\n",
      "Epoch [100]/[200] running accumulative loss across all batches: 2.199\n",
      "Test err: 10.659667536616325\n",
      "Epoch [101]/[200] running accumulative loss across all batches: 2.180\n",
      "Test err: 10.582511286251247\n",
      "Epoch [102]/[200] running accumulative loss across all batches: 2.162\n",
      "Test err: 10.506684833206236\n",
      "Epoch [103]/[200] running accumulative loss across all batches: 2.144\n",
      "Test err: 10.432159630116075\n",
      "Epoch [104]/[200] running accumulative loss across all batches: 2.126\n",
      "Test err: 10.35893221758306\n",
      "Epoch [105]/[200] running accumulative loss across all batches: 2.108\n",
      "Test err: 10.286976161878556\n",
      "Epoch [106]/[200] running accumulative loss across all batches: 2.091\n",
      "Test err: 10.216267638839781\n",
      "Epoch [107]/[200] running accumulative loss across all batches: 2.073\n",
      "Test err: 10.146795230917633\n",
      "Epoch [108]/[200] running accumulative loss across all batches: 2.056\n",
      "Test err: 10.078540812712163\n",
      "Epoch [109]/[200] running accumulative loss across all batches: 2.039\n",
      "Test err: 10.011489149648696\n",
      "Epoch [110]/[200] running accumulative loss across all batches: 2.022\n",
      "Test err: 9.945619912585244\n",
      "Epoch [111]/[200] running accumulative loss across all batches: 2.006\n",
      "Test err: 9.880920675001107\n",
      "Epoch [112]/[200] running accumulative loss across all batches: 1.989\n",
      "Test err: 9.817375960585196\n",
      "Epoch [113]/[200] running accumulative loss across all batches: 1.973\n",
      "Test err: 9.754968015884515\n",
      "Epoch [114]/[200] running accumulative loss across all batches: 1.957\n",
      "Test err: 9.693674375273986\n",
      "Epoch [115]/[200] running accumulative loss across all batches: 1.941\n",
      "Test err: 9.63348219380714\n",
      "Epoch [116]/[200] running accumulative loss across all batches: 1.925\n",
      "Test err: 9.574218251647835\n",
      "Epoch [117]/[200] running accumulative loss across all batches: 1.909\n",
      "Test err: 9.515837016130263\n",
      "Epoch [118]/[200] running accumulative loss across all batches: 1.893\n",
      "Test err: 9.458868428266214\n",
      "Epoch [119]/[200] running accumulative loss across all batches: 1.878\n",
      "Test err: 9.40293758840562\n",
      "Epoch [120]/[200] running accumulative loss across all batches: 1.863\n",
      "Test err: 9.34803502025295\n",
      "Epoch [121]/[200] running accumulative loss across all batches: 1.848\n",
      "Test err: 9.294143023216748\n",
      "Epoch [122]/[200] running accumulative loss across all batches: 1.833\n",
      "Test err: 9.241248396072478\n",
      "Epoch [123]/[200] running accumulative loss across all batches: 1.818\n",
      "Test err: 9.189332992391428\n",
      "Epoch [124]/[200] running accumulative loss across all batches: 1.804\n",
      "Test err: 9.138390672276728\n",
      "Epoch [125]/[200] running accumulative loss across all batches: 1.789\n",
      "Test err: 9.088398286432493\n",
      "Epoch [126]/[200] running accumulative loss across all batches: 1.775\n",
      "Test err: 9.039129099226557\n",
      "Epoch [127]/[200] running accumulative loss across all batches: 1.761\n",
      "Test err: 8.991008874727413\n",
      "Epoch [128]/[200] running accumulative loss across all batches: 1.747\n",
      "Test err: 8.943803201895207\n",
      "Epoch [129]/[200] running accumulative loss across all batches: 1.733\n",
      "Test err: 8.897496503777802\n",
      "Epoch [130]/[200] running accumulative loss across all batches: 1.719\n",
      "Test err: 8.852073178859428\n",
      "Epoch [131]/[200] running accumulative loss across all batches: 1.706\n",
      "Test err: 8.80752930114977\n",
      "Epoch [132]/[200] running accumulative loss across all batches: 1.692\n",
      "Test err: 8.763845413923264\n",
      "Epoch [133]/[200] running accumulative loss across all batches: 1.679\n",
      "Test err: 8.721005875151604\n",
      "Epoch [134]/[200] running accumulative loss across all batches: 1.666\n",
      "Test err: 8.6790079777129\n",
      "Epoch [135]/[200] running accumulative loss across all batches: 1.653\n",
      "Test err: 8.637829823885113\n",
      "Epoch [136]/[200] running accumulative loss across all batches: 1.640\n",
      "Test err: 8.597467907704413\n",
      "Epoch [137]/[200] running accumulative loss across all batches: 1.627\n",
      "Test err: 8.557901559397578\n",
      "Epoch [138]/[200] running accumulative loss across all batches: 1.614\n",
      "Test err: 8.519130372442305\n",
      "Epoch [139]/[200] running accumulative loss across all batches: 1.602\n",
      "Test err: 8.481135284528136\n",
      "Epoch [140]/[200] running accumulative loss across all batches: 1.590\n",
      "Test err: 8.443899156525731\n",
      "Epoch [141]/[200] running accumulative loss across all batches: 1.577\n",
      "Test err: 8.40699118282646\n",
      "Epoch [142]/[200] running accumulative loss across all batches: 1.565\n",
      "Test err: 8.371265904046595\n",
      "Epoch [143]/[200] running accumulative loss across all batches: 1.553\n",
      "Test err: 8.336273165419698\n",
      "Epoch [144]/[200] running accumulative loss across all batches: 1.541\n",
      "Test err: 8.302000411786139\n",
      "Epoch [145]/[200] running accumulative loss across all batches: 1.529\n",
      "Test err: 8.268438130617142\n",
      "Epoch [146]/[200] running accumulative loss across all batches: 1.518\n",
      "Test err: 8.235577002167702\n",
      "Epoch [147]/[200] running accumulative loss across all batches: 1.506\n",
      "Test err: 8.20330079831183\n",
      "Epoch [148]/[200] running accumulative loss across all batches: 1.495\n",
      "Test err: 8.171811012551188\n",
      "Epoch [149]/[200] running accumulative loss across all batches: 1.483\n",
      "Test err: 8.140986926853657\n",
      "Epoch [150]/[200] running accumulative loss across all batches: 1.472\n",
      "Test err: 8.11082237586379\n",
      "Epoch [151]/[200] running accumulative loss across all batches: 1.461\n",
      "Test err: 8.081307431682944\n",
      "Epoch [152]/[200] running accumulative loss across all batches: 1.450\n",
      "Test err: 8.05243424884975\n",
      "Epoch [153]/[200] running accumulative loss across all batches: 1.439\n",
      "Test err: 8.024188006296754\n",
      "Epoch [154]/[200] running accumulative loss across all batches: 1.428\n",
      "Test err: 7.996563360095024\n",
      "Epoch [155]/[200] running accumulative loss across all batches: 1.418\n",
      "Test err: 7.969454437494278\n",
      "Epoch [156]/[200] running accumulative loss across all batches: 1.407\n",
      "Test err: 7.94304421171546\n",
      "Epoch [157]/[200] running accumulative loss across all batches: 1.397\n",
      "Test err: 7.917227549478412\n",
      "Epoch [158]/[200] running accumulative loss across all batches: 1.386\n",
      "Test err: 7.891997219994664\n",
      "Epoch [159]/[200] running accumulative loss across all batches: 1.376\n",
      "Test err: 7.867339795455337\n",
      "Epoch [160]/[200] running accumulative loss across all batches: 1.366\n",
      "Test err: 7.843247433193028\n",
      "Epoch [161]/[200] running accumulative loss across all batches: 1.356\n",
      "Test err: 7.819712119176984\n",
      "Epoch [162]/[200] running accumulative loss across all batches: 1.346\n",
      "Test err: 7.7967303562909365\n",
      "Epoch [163]/[200] running accumulative loss across all batches: 1.336\n",
      "Test err: 7.774286236613989\n",
      "Epoch [164]/[200] running accumulative loss across all batches: 1.326\n",
      "Test err: 7.7523756278678775\n",
      "Epoch [165]/[200] running accumulative loss across all batches: 1.317\n",
      "Test err: 7.730988988652825\n",
      "Epoch [166]/[200] running accumulative loss across all batches: 1.307\n",
      "Test err: 7.7101172879338264\n",
      "Epoch [167]/[200] running accumulative loss across all batches: 1.298\n",
      "Test err: 7.689755382016301\n",
      "Epoch [168]/[200] running accumulative loss across all batches: 1.288\n",
      "Test err: 7.66989508876577\n",
      "Epoch [169]/[200] running accumulative loss across all batches: 1.279\n",
      "Test err: 7.650527503807098\n",
      "Epoch [170]/[200] running accumulative loss across all batches: 1.270\n",
      "Test err: 7.631647421978414\n",
      "Epoch [171]/[200] running accumulative loss across all batches: 1.261\n",
      "Test err: 7.6132422857917845\n",
      "Epoch [172]/[200] running accumulative loss across all batches: 1.252\n",
      "Test err: 7.5951859396882355\n",
      "Epoch [173]/[200] running accumulative loss across all batches: 1.243\n",
      "Test err: 7.577718775952235\n",
      "Epoch [174]/[200] running accumulative loss across all batches: 1.234\n",
      "Test err: 7.560706716263667\n",
      "Epoch [175]/[200] running accumulative loss across all batches: 1.225\n",
      "Test err: 7.544144879095256\n",
      "Epoch [176]/[200] running accumulative loss across all batches: 1.216\n",
      "Test err: 7.528024872764945\n",
      "Epoch [177]/[200] running accumulative loss across all batches: 1.208\n",
      "Test err: 7.5123375174589455\n",
      "Epoch [178]/[200] running accumulative loss across all batches: 1.199\n",
      "Test err: 7.497079031076282\n",
      "Epoch [179]/[200] running accumulative loss across all batches: 1.191\n",
      "Test err: 7.48224182787817\n",
      "Epoch [180]/[200] running accumulative loss across all batches: 1.183\n",
      "Test err: 7.467819359037094\n",
      "Epoch [181]/[200] running accumulative loss across all batches: 1.174\n",
      "Test err: 7.45380600594217\n",
      "Epoch [182]/[200] running accumulative loss across all batches: 1.166\n",
      "Test err: 7.440193655434996\n",
      "Epoch [183]/[200] running accumulative loss across all batches: 1.158\n",
      "Test err: 7.426976535614813\n",
      "Epoch [184]/[200] running accumulative loss across all batches: 1.150\n",
      "Test err: 7.414147581206635\n",
      "Epoch [185]/[200] running accumulative loss across all batches: 1.142\n",
      "Test err: 7.401704056697781\n",
      "Epoch [186]/[200] running accumulative loss across all batches: 1.134\n",
      "Test err: 7.389635349616583\n",
      "Epoch [187]/[200] running accumulative loss across all batches: 1.126\n",
      "Test err: 7.377938613812148\n",
      "Epoch [188]/[200] running accumulative loss across all batches: 1.119\n",
      "Test err: 7.36660623410171\n",
      "Epoch [189]/[200] running accumulative loss across all batches: 1.111\n",
      "Test err: 7.355631748314863\n",
      "Epoch [190]/[200] running accumulative loss across all batches: 1.104\n",
      "Test err: 7.34501141477449\n",
      "Epoch [191]/[200] running accumulative loss across all batches: 1.096\n",
      "Test err: 7.33473784691887\n",
      "Epoch [192]/[200] running accumulative loss across all batches: 1.089\n",
      "Test err: 7.324805568903685\n",
      "Epoch [193]/[200] running accumulative loss across all batches: 1.081\n",
      "Test err: 7.315209463456995\n",
      "Epoch [194]/[200] running accumulative loss across all batches: 1.074\n",
      "Test err: 7.305945506086573\n",
      "Epoch [195]/[200] running accumulative loss across all batches: 1.067\n",
      "Test err: 7.2969445083872415\n",
      "Epoch [196]/[200] running accumulative loss across all batches: 1.060\n",
      "Test err: 7.288329515606165\n",
      "Epoch [197]/[200] running accumulative loss across all batches: 1.052\n",
      "Test err: 7.280027171364054\n",
      "Epoch [198]/[200] running accumulative loss across all batches: 1.045\n",
      "Test err: 7.272034379420802\n",
      "Epoch [199]/[200] running accumulative loss across all batches: 1.039\n",
      "Test err: 7.264346691896208\n",
      "Epoch [200]/[200] running accumulative loss across all batches: 1.032\n",
      "Test err: 7.256959788966924\n",
      "Training is finishing!\n"
     ]
    }
   ],
   "source": [
    "net = FeedForward(8, 24, 12, 6)\n",
    "SGD_res = train(optimizer=torch.optim.SGD(net.parameters(), lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты модели с различными оптимайзерами (Ошибка MSE на тесте): \n",
      "Adam: 41.310118943452835 \n",
      "RMSprop: 10.509002172388136 \n",
      "SGD: 7.256959788966924\n"
     ]
    }
   ],
   "source": [
    "print(f'Результаты модели с различными оптимайзерами (Ошибка MSE на тесте): \\n'\n",
    "      f'Adam: {Adam_res} \\n'\n",
    "      f'RMSprop: {RMSprop_res} \\n'\n",
    "      f'SGD: {SGD_res}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего себя показал оптимайзер SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
